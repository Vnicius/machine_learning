#modify backprop to build one-layer auto-coder
cd /home/mike-bowles/octave/simpleMNist
clear
#load some text into a matrix
load zero.txt;
zero = zero + 1;
load one.txt
one = one + 1;

#the entries in these matrices span [0,2].  
#to retain logistic output neuron rescale to [0,1]

one = one ./ 2.0;
zero = zero ./ 2.0;


#define activation function (logistic)
function retval = phi (z)
retval = 1 ./ (1+exp(-z));
endfunction


#Network Definition
#
#256 inputs
#128 hidden layer
#256 output layer (0,1)
#fully connected feed-forward

#Notations:
#W1(128,256) - weight matrix from input to hidden layer
#b1(128) - biases at input layer
#W2(256,128) - weight matrix from hidden to output layer
#b2(256) - biases at layer 2
#suppose logistic neurons in hidden and output layers
#z2 - W1*input' + b1 (128 column vector)
#a2 - f(z2)  (128 column vector)
#z3 - W2*a2 + b2  (256 vector)
#a3 - f(z3)  (256 vector)
#d3 =-(yi - a3i) .* f'(z3)  (256 -vector - component-wise product of yi - a3i and f'(z3))
#
#d2 = (W2' d3) .* f'(z2)

#initialize weights and biases
W1 = unifrnd(-1.0, 1.0, 128, 256);
b1 = unifrnd(-1.0, 1.0, 128, 1);
W2 = unifrnd(-1.0, 1.0, 256, 128);
b2 = unifrnd(-1.0, 1.0, 256, 1);
#pick a step-size
eps = 0.5;
lambda = 1.0;

#Begin Iteration
#form zero matrices in which to accumulate increments
dW1 = zeros(128, 256);
db1 = zeros(128,1);
dW2 = zeros(256,128);
db2 = zeros(256,1);

#use these to accumulate rss error for convergence visability
error0 = zeros(rows(zero),1);
error1 = zeros(rows(one),1);



#batch gradient descent
########################

#start with zero examples
#one example at a time.  


for i = 1:rows(zero)
#test case Xin = transpose(zero(1,:));
Xin = transpose(zero(i,:));

#forward pass
a1 = Xin;
z2 = W1*Xin + b1;
a2 = phi(z2);
z3 = W2*a2 + b2;
a3 = phi(z3);
error0(i) = (1/256)*sum((Xin-a3).*(Xin-a3));

#backprop - calc deltas
d3 = -(Xin - a3) .* a3 .* (1-a3);
d2 = transpose(W2)*d3 .* a2 .*(1-a2);

dW1 = dW1 + d2*transpose(a1);
db1 = db1 + d2;

dW2 = dW2 + d3*transpose(a2);
db2 = db2 + d3;
end

for i = 1:rows(one)
#test case Xin = transpose(one(1,:));
Xin = transpose(one(i,:));

#forward pass
a1 = Xin;
z2 = W1*Xin + b1;
a2 = phi(z2);
z3 = W2*a2 + b2;
a3 = phi(z3);
error1(i) = (1/256)*sum((Xin-a3).*(Xin-a3));

#backprop - calc deltas
d3 = -(Xin - a3) .* a3 .* (1-a3);
d2 = transpose(W2)*d3 .* a2 .*(1-a2);

dW1 = dW1 + d2*transpose(a1);
db1 = db1 + d2;

dW2 = dW2 + d3*transpose(a2);
db2 = db2 + d3;
end

##
#Weight Update Eqn


############################################
#regularized weight updates
##########################################
#Update weights
m = rows(zero) + rows(one);
W1 = W1 - eps*((1/m)*dW1 + lambda*W1);
b1 = b1 - eps*(1/m)*db1;
W2 = W2 - eps*((1/m)*dW2 + lambda*W2);
b2 = b2 - eps*(1/m)*db2;

#check convergence
norm(dW1*(1/m) + lambda*W1) 
norm(db1*(1/m))
norm(dW2*(1/m) + lambda*W2)
norm(db2*(1/m))
sqrt(sum(error0)/rows(error0))
sqrt(sum(error1)/rows(error1))

#End Iteration

iTry = 1;

#Weight visualization
wImage = zeros(16,16);
wTemp = W1(iTry,:);
for i = 1:16
wImage(i,:) = wTemp((1 + (i-1)*16):i*16);
end
imagesc(wImage), colorbar, colormap gray

############################################
#regularized weight updates
##########################################
#Update weights
m = rows(zero) + rows(one);
W1 = W1 - eps*((1/m)*dW1 + lambda*W1);
b1 = b1 - eps*(1/m)*db1;
W2 = W2 - eps*((1/m)*dW2 + lambda*W2);
b2 = b2 - eps*(1/m)*db2;

#check convergence
norm(dW1*(1/m) + lambda*W1) 
norm(db1*(1/m))
norm(dW2*(1/m) + lambda*W2)
norm(db2*(1/m))
norm(error0)
norm(error1)


########################################### 
#Update weights wo weight penalty term
############################################
m = rows(zero) + rows(one);
W1 = W1 - eps*(1/m)*dW1;
b1 = b1 - eps*(1/m)*db1;
W2 = W2 - eps*(1/m)*dW2;
b2 = b2 - eps*(1/m)*db2;

#check convergence
norm(dW1*(1/m)) 
norm(db1*(1/m))
norm(dW2*(1/m))
norm(db2*(1/m))
norm(error0)
norm(error1)